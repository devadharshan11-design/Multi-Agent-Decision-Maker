Accelerating YOLOv8n on Raspberry Pi: An Empirical Study of Structured Pruning and Fine-tuning 1st Devadharshan D Department of Mechanical Engineering Amrita School of Engineering, Coimbatore Amrita Vishwa Vidyapeetham, India cb.en.u4are22014@cb.students.amrita.edu 2nd Karthika R Department of Electronics and Communication Engineering Amrita School of Engineering, Coimbatore Amrita Vishwa Vidyapeetham, India r karthika@cb.amrita.edu Abstract—Deploying state-of-the-art object detectors on resource-constrained edge hardware remains a significant chal- lenge due to computational bottlenecks. This paper presents a novel, end-to-end hybrid pipeline for compressing the YOLOv8n model specifically for the Raspberry Pi 5. Unlike generic opti- mization methods, our approach integrates surgical structural pruning to target and remove redundant bottleneck layers, followed by fine-tuning for accuracy recovery and post-training INT8 quantization. Empirical benchmarks demonstrate that this pipeline achieves an inference speed of 4.13 FPS, a 34% acceleration over the baseline, with a negligible accuracy drop (0.354 mAP vs. 0.355 mAP). This work validates a reproducible blueprint for high-performance edge inference, bridging the gap between theoretical compression and real-world deployment. Keywords—Edge AI, Model Compression, YOLOv8, Object Detection, Raspberry Pi. I. INTRODUCTION The increasing demand for real-time intelligence in edge computing necessitates the deployment of deep learning mod- els on resource-constrained hardware. While state-of-the-art object detectors like YOLOv8n offer high accuracy, their computational demands often render them unsuitable for direct use on low-cost platforms such as the Raspberry Pi, where in- ference latency is a critical bottleneck. Although compression techniques like pruning and quantization exist, there remains a lack of practical, end-to-end benchmarks that evaluate their real-world impact on modern architectures and hardware. To address this gap, this paper presents a hybrid pipeline that combines surgical pruning of the YOLOv8n architecture, performance recovery through fine-tuning, and post-training INT8 quantization to accelerate inference on a Raspberry Pi 5. The main contributions of this work are: •A specific surgical pruning methodology to reduce the architectural depth of YOLOv8n by targeting redundant bottleneck blocks; •A demonstration of near-complete accuracy recovery (99.7%) via fine-tuning; and •A reproducible, empirical benchmark on the Raspberry Pi 5, achieving a 34% speedup (4.13 FPS) with negligible accuracy loss. II. LITERATUREREVIEW Applications ranging from industrial automation to preci- sion agriculture depend on the deployment of deep learning models on edge devices with limited resources. Despite pro- viding cutting-edge accuracy, YOLOv8’s computational cost necessitates extensive optimization for real-time embedded use. Three main areas are the focus of current research: hardware-focused deployment studies, enhanced small-object detection, and lightweight architectural redesign. Recent research has substituted lightweight convolutions for standard convolutions in order to lower parameters and FLOPs. For example, [1] and [2] achieved significant parame- ter reductions by integrating Ghost convolutions and attention mechanisms (M-CBAM, RepGhost) into YOLO architectures for agricultural applications. Others have focused on backbone and neck optimization; [3] reduced computational costs by 35.8% using Reversible Column Networks and GSConv, while [4] employed a lightweight GELAN and Efficient Channel Attention to enhance maritime detection efficiency. [5] developed LACF-YOLO, which integrates Triplet At- tention and Dilated Inverted Bottlenecks to reduce feature loss, in order to tackle the difficulties of small object detection. In a similar vein, [6] proposed SOD-YOLOv8, which uses Powerful-IoU loss and a fourth detection layer to preserve spatial details in traffic scenarios. Although these multi-scale fusion methods successfully improve accuracy, their compu- tational overhead makes deployment on limited edge devices more difficult. Because of hardware constraints, real-world deployment fre- quently negates architectural gains. Unoptimized models have substantial latency on Raspberry Pi devices, with inference times surpassing 800 ms on Pi 4, according to studies [7] and [8]. Moreover, complex hybrid models [9] continue to be computationally prohibitive. In response, [10] proposed YOLOv8n-FAWL, which shows that real-time performance (54 FPS) on edge hardware can be achieved by reducing pa- rameters by 49.2% by combining Layer-Adaptive Magnitude Pruning with sophisticated loss functions.
<<CHUNK>>
Additionally, pruning and knowledge transfer have been combined in recent work. In order to enable the pruned student model to learn rich features from a larger teacher, [11] proposed PKD-YOLOv8 for pest detection on edge devices, combining structured pruning with Logit-based Masked Gen- erative Distillation. This method demonstrated the efficacy of hybrid pruning–distillation techniques for edge deployment by reducing model size by 60.7% (11.2 MB to 4.4 MB) with only a 0.1% accuracy drop.A Convultional Neural Network(CNN)- based traffic sign recognition system on Raspberry Pi [12] reached 99.8% accuracy on the GTSRB dataset, demonstrating the viability of embedded vision. A two-stage YOLOv3 + CNN pipeline [13] achieved 89.56% mAP and 86.6% accuracy under varied conditions. Likewise, a stereo-vision pseudo- LiDAR model [14] enabled accurate 3D perception for real- time path planning, and an IMU-based driver-behavior classi- fication system [15] achieved 98% accuracy and introduced a DriveScore for safety assessment. There is a dichotomy in the literature today: either standard models with inadequate latency on low-power hardware ( [7]) or complex architectures that are too heavy for edge devices ( [5], [6], [9]). There is a significant need for a rigorous, repeatable pipeline that combines INT8 quantization, fine-tuning, and surgical structural pruning that is specifically benchmarked on the Raspberry Pi 5. In order to close this gap, this work empirically validates a hybrid pipeline that allows for real-time inference while maintaining state-of-the-art accuracy. III. METHODOLOGY This work employs a sequential pipeline to optimize YOLOv8n for edge hardware. The methodology consists of five phases: baseline establishment, surgical pruning, accuracy recovery, quantization, and final deployment. A. Image Acquisition The pipeline accepts raw visual data from the COCO 2017 dataset for training. Input images are preprocessed by resizing them to a standardized resolution of 640 × 640 pixels to match the network’s input layer. Pixel values are normalized to the range [0, 1]. B. YOLOv8n Architecture Overview The standardYOLOv8n (Nano)architecture, selected for its balance between accuracy and computational efficiency, is used to initialize the optimization pipeline. To understand the optimization strategy, it is essential to define the specific building blocks of this architecture: 1) CSPDarknet53 Backbone:The backbone of YOLOv8n is derived from theCSPDarknet53family, serving as the primary feature extractor of the network. It processes the input image layer by layer using strided convolutions to produce hierarchical feature maps at three spatial scales( P3, P4, P5).These multi-scale features enable robust detection of objects of varying sizes. 2) C2f Module (Cross-Stage Partial with Two Convolu- tions):YOLOv8 replaces the earlier C3 module with the more efficientC2f(Cross-Stage Partial with two convolutions) module. The design improves gradient flow and reduces com- putational overhead. Given an input tensor, the module splits it into two paths: •Path 1: Passes through a sequence of bottleneck layers. •Path 2: Bypasses the bottlenecks entirely. The outputs of both paths are then concatenated.This design strengthens gradient propagation during backpropagation, re- duces redundancy, and improves the representational power of the lightweight YOLOv8n network. 3) Bottleneck Layer:ABottleneckis a lightweight sub- unit within the C2f module designed to reduce computational complexity. It operates by first compressing the channel di- mensions, performing internal transformations efficiently, and then expanding the channels back to their original size. Stack- ing multiple Bottleneck layers increases both the effective network depth and the receptive field, allowing the model to capture richer hierarchical features. 4) SPPF (Spatial Pyramid Pooling – Fast):At the end of the backbone, YOLOv8 employs theSPPFmodule, a faster variant of Spatial Pyramid Pooling (SPP). This module performs max pooling at multiple kernel sizes, such as: (5×5, 9×9,13×13). The pooled outputs are concatenated, enabling the model to aggregate multi-scale context. This operation enhances robustness to object scale variations and improves global information capture without significant computational overhead. 5) PANet (Path Aggregation Network):ThePANetarchi- tecture is used as the “Neck” of YOLOv8. It aggregates and fuses features from different stages of the backbone to improve detection accuracy. PANet combines: •High-level semantic featuresfrom deeper layers •Low-level localization featuresfrom shallow layers This top-down and bottom-up fusion ensures that the final feature maps are both semantically rich and spatially precise, which is crucial for accurate object detection across scales. 6) Optimization Hypothesis:While the standard C2f mod- ule maximizes representational capacity by stacking multiple Bottleneck layers, we hypothesize that,
<<CHUNK>>
for specific edge deployment scenarios, thefinal Bottleneckin the stack may introduce computational redundancy. This additional Bottle- neck contributes to increased latency without providing a proportional improvement in feature discrimination. Therefore, removing or pruning this Bottleneck may retain accuracy while significantly reducing inference time. C. Surgical Structural Pruning A Structured Surgical Pruning algorithm was created to reduce the computational latency present in deep convolutional networks on edge hardware. Our method physically alters the model graph by eliminating entire architectural blocks, in contrast to unstructured pruning, which zeros out individual weights to create sparse matrices (often requiring specialized
<<CHUNK>>
Fig. 1: Schematic of the Surgical Pruning process on a C2f module. (a) The standard module architecture (b) The surgically pruned module where the redundant bottleneck is removed Algorithm 1Surgical Pruning of C2f Modules Require:Pre-trained YOLOv8n ModelM Ensure:Pruned ModelM pruned 1:foreach moduleminM.modules()do 2:ifmis instance ofC2fthen 3:n←len(m.bottlenecks) 4:ifn >1then 5:{Remove the final redundant bottleneck} 6:m.bottlenecks←m.bottlenecks[:−1] 7:{Calculate new input channels for stability} 8:C skip ←m.cv1.out channels 9:C proc ←len(m.bottlenecks)× m.bottlenecks[0].out 10:C new ←C skip +C proc 11:{Rebuild final convolution} 12:m.cv2←Conv(C new, m.cv2.out, ...) 13:end if 14:end if 15:end for 16:returnM hardware support to realize speed gains). This leads to a lighter, denser model that offers universal acceleration on common CPUs, such as the Arm Cortex-A76 in the Raspberry Pi 5. 1) Target Identification: The C2f Module:The pruning process specifically targets theC2f(Cross-Stage Partial with two convolutions) module, which serves as the primary feature extraction block within the YOLOv8 backbone. As illustrated in Fig. 2(a), the standard C2f module splits the incoming tensor into two parallel computational paths to enhance gradient flow and feature diversity. •Skip Path:A direct shortcut branch that preserves low- level spatial features by forwarding a portion of the input tensor unchanged. •Processing Path:A deeper branch composed of a se- quence of Bottleneck layers (denoted asBottleneck 1 andBottleneck 2in the figure), responsible for extracting higher-level semantic representations. Our analysis hypothesizes that, for specific edge-focused detection tasks, the additional depth introduced by the final Bottleneck layer contributes disproportionately to computa- tional load while offering only marginal improvements in feature discrimination. Therefore, this redundant Bottleneck becomes the primary candidate for structural pruning in order to reduce inference latency without sacrificing model accuracy. 2) Algorithmic Implementation:The pruning algorithm fol- lows a three-stageIdentify–Remove–Repairworkflow that op- erates introspectively on the PyTorch computation graph: a) Step A: Introspection of the Module:The prun- ing script iterates through them.Modulehierarchy of the YOLOv8n model to locate every instance of theC2fclass. Each C2f block contains an internal attributem, implemented as anm.ModuleList, which stores the sequential Bottle- neck layers. Blocks with a depth ofn >1are marked as candidates for structural reduction. b) Step B: Removal by Surgery:For modules identified as over-parameterized, the script performs a structural mod- ification by removing the final Bottleneck layer through a Python slice operation on theModuleList. This pruning step is visually represented in Fig. 2(b) by the crossed-out “REMOVED” block. The elimination of this layer immedi- ately reduces the block’s FLOPs and parameter count, thereby lowering computational cost during inference. c) Step C: Reconstructing the Dynamic Graph:Re- moving a single Bottleneck layer disrupts the computational
<<CHUNK>>
graph, since the final convolutional layer (cv2) expects a concatenated tensor composed of the Skip Path output and the outputs of all Bottleneck layers. WhenBottleneck 2is removed, the concatenated feature tensor becomes smaller, producing a channel-dimension mismatch. To restore graph integrity, the pruning algorithm performs aDynamic Reconnection(depicted by the curved arrow in Fig. 2(b)). The correct input dimensionality for the re- instantiatedcv2layer is computed as: Cin,new =C 1 + (nremaining ×C out,bottleneck)(1) whereC 1 denotes the channel count from the Skip Path andn remaining represents the number of Bottleneck layers that remain after pruning. Following this calculation, the originalcv2layer is de- stroyed and reconstructed with the updated input channel dimension, thereby “healing” the module and ensuring that the forward-pass data flow remains valid during inference. D. Accuracy Recovery via Fine-Tuning The ”lobotomy” effect caused by the surgical removal of layers severely impairs the model’s ability to extract features and initially lowers the mean Average Precision (mAP) to almost zero. We used a fine-tuning approach to restore this performance. Although Knowledge Distillation (KD), which involves a heavy ”Teacher” model supervising the pruned ”Student,” is a popular technique for this stage, our empirical analysis showed that it is computationally unnecessary for this particular task. We found that 99.7% of the baseline accuracy could be recov- ered using a direct Fine-Tuning protocol. This suggests that the pruned architecture still has enough capacity to learn the features of the dataset without a teacher’s soft-label guidance. •Training Protocol:The pruned model is handled like a student model who has already received training. In order to preserve low-level feature extractors (such as edge and texture filters), we use Transfer Learning principles by freezing the backbone’s initial layers. Using the Stochastic Gradient Descent (SGD) optimizer with a momentum of 0.937 and an initial learning rate of 0.01, the modified C2f modules and the detection head are unfrozen and retrained on the COCO 2017 dataset for 50 epochs. E. Post-Training Quantization (PTQ) To maximize inference throughput on the Raspberry Pi’s CPU, the recovered model undergoes Post-Training Quantiza- tion. This process maps the model’s parameters from high- precision 32-bit floating-point (FP32) representation to lower- precision 8-bit integers (INT8). •Quantization Mapping: The weights (W) and activation maps (A) are quantized using an affine mapping defined by a scale factorSand a zero-pointZ. The quantization functionQ(x)is defined as: Q(x) =clamp  round  x S  +Z,−128,127  (2) wherexrepresents the FP32 value. The scale factorS is calibrated by passing a representative dataset through the network to determine the dynamic range [min,max] of activations at each layer. •Hardware Impact: This transformation reduces the model’s memory foot- print by a factor of4. Furthermore, it enables the inference engine to utilize the Raspberry Pi’s integer arithmetic units, which are significantly faster and more energy-efficient than floating-point units, directly allevi- ating memory bandwidth bottlenecks. F . Edge Inference Implementation The TensorFlow Lite Runtime with the XNNPACK delegate enabled is used to deploy the optimized.tflite model on the Raspberry Pi 5 (Broadcom BCM2712, Cortex-A76). XN- NPACK guarantees that theoretical reductions in FLOPs from pruning and quantization directly translate into lower inference latency by utilizing optimized ARM64 NEON assembly for neural operators. G. Justification for Pruning Strategy Our ’surgical’ approach focuses on architectural depth, in contrast to unstructured pruning, which frequently fails to accelerate standard CPUs despite high sparsity. We minimize sequential floating-point operations (FLOPs) by removing entire bottleneck layers. Because the speedup results from a shorter execution path rather than merely a smaller model, this explains how a modest 3.5% parameter reduction produces quantifiable FPS gains. IV. SYSTEMARCHITECTURE In order to convert the computationally costly YOLOv8n model into an effective edge-ready format, the suggested system architecture functions as a sequential optimization pipeline. The following steps make up the process flow, which is depicted in Figure 2: •Input Data:Raw visual data is accepted by the pipeline. This includes using the COCO 2017 dataset for training during the optimization stage. •Surgical Pruning:The pruning engine receives the base- line model, introspects the architecture, and programmat- ically eliminates unnecessary bottleneck layers from the C2f modules in order to decrease model depth. •Fine-Tuning:The retraining module receives the pruned model. It goes through 50 epochs of fine-tuning on the target dataset, which enables
<<CHUNK>>
the remaining weights to adjust and regain accuracy. •Quantization:Post-Training Quantization (PTQ) trans- forms the recovered floating-point (FP32) model into an 8-bit integer (INT8) format in order to optimize execution speed on the edge CPU. •Output:The TFLite runtime with XNNPACK acceler- ation is used to deploy the final optimized model on the Raspberry Pi 5. Bounding boxes and class labels are among the real-time detection predictions that the system generates.
<<CHUNK>>
Fig. 2: Block diagram of the proposed End-to-End Compression and Deployment Pipeline. V. EXPERIMENTALSETUP To ensure reproducibility, all models were trained on the COCO 2017dataset. A. Hardware & Protocols Optimization tasks were performed on a workstation equipped with an Intel Core i7-12700H CPU and NVIDIA GeForce RTX 3060 GPU, utilizing PyTorch 2.0.1. Final bench- marking was conducted on aRaspberry Pi 5(4GB RAM, Broadcom BCM2712) running Raspberry Pi OS (64-bit). The pruned model was fine-tuned for 50 epochs (Batch=16, ImgSz=640). B. Evaluation Metrics Performance was evaluated using: •mAP@.50-.95:Primary accuracy metric on COCO val2017. •FPS:Average inference speed over 100 continuous frames. •Parameter Count:Model complexity and size. VI. RESULTS ANDDISCUSSION This section presents the empirical evaluation of our pro- posed compression pipeline. We compare the final optimized model against the original YOLOv8n baseline across key metrics of model size, detection accuracy, and inference speed on the target hardware. A. Comparison with Standard Techniques To demonstrate the efficacy of our hybrid pipeline, we compared it against standard optimization methods (Table I). TABLE I: Comparison with Standard Optimization Methods Method mAP Drop FPS Gain Standard PTQ (INT8 only) -0.2% +25% Unstructured Pruning (50%) -1.5% +0% (CPU) Ours (Surgical + INT8) -0.3% +34% As shown, standard INT8 quantization alone provides a speedup but misses the architectural efficiency gains. Unstruc- tured pruning reduces size but fails to accelerate inference on the Raspberry Pi CPU. Our hybrid approach provides the highest FPS gain by combining architectural reduction with hardware-aware quantization. B. Ablation Study To validate the contribution of each stage in our pipeline, we conducted a step-by-step ablation study. Table II details the impact of each optimization phase on model size, accuracy, and inference speed. TABLE II: Ablation Study of the Compression Pipeline Pipeline Stage Params mAP FPS (Pi 5) 1. Baseline (FP32) 3.16M 0.355 3.08 2. Surgical Pruning Only 3.05M 0.004 - 3. Pruning + Fine-Tuning 3.05M 0.354 3.12 4. Full Pipeline (+INT8) 3.05M 0.354 4.13 Analysis: •Step 1 (Baseline):The uncompressed FP32 model served as the reference, achieving 3.08 FPS. This low speed confirmed the need for optimization on the Raspberry Pi 5. •Step 2 (Pruning):Directly removing the bottlenecks caused a catastrophic drop in accuracy (0.004 mAP), validating that the removed layers contained learned features that needed to be recovered. •Step 3 (Fine-Tuning):Retraining successfully recovered 99.7% of the original accuracy, validating that the pruned architecture had sufficient capacity to learn the task. •Step 4 (Quantization):The shift to INT8 provided the largest speed gain (32% increase over FP32) with zero additional accuracy loss. C. Quantitative Metrics The core performance of the compression pipeline is sum- marized in Table I. The baseline YOLOv8n model, with 3.16M parameters, achieved an inference speed of 3.08 FPS on the Raspberry Pi 5. After applying the full pipeline of surgical pruning, fine-tuning, and INT8 quantization, the final optimized model contains 3.05M parameters. TABLE III: Comparison of Baseline and Optimized YOLOv8n Models Model Version Parameters mAP@.5-.95 FPS (Pi 5) Baseline YOLOv8n 3.16M 0.355 3.08 Optimized Model 3.05M 0.354 4.13 As presented in Table III, the surgical pruning and rebuild- ing of the C2f modules resulted in a final model with 3.05M parameters, a reduction of 3.5% from the 3.16M baseline. The critical outcome is the inference speed on the Raspberry Pi 5, which increased from 3.08 FPS to 4.13 FPS, representing a significant 34% acceleration. This performance gain was
<<CHUNK>>
achieved with a negligible 0.3% degradation in accuracy, with the final model scoring 0.354 mAP@.5-.95. These results demonstrate a highly successful trade-off, where a substantial improvement in computational efficiency is realized with a minimal impact on the model’s detection capabilities. Fig. 3: Performance trade-off: Inference Speed vs. Accuracy To visualize this efficiency gain, the relationship between inference speed and detection accuracy is plotted in Fig. 3. The vector in the plot indicates a clear shift towards higher efficiency (rightward movement) with minimal vertical drop, validating that the ”surgical” nature of the pruning successfully targeted redundant computations rather than critical feature ex- tractors. Visual examination was done on the model’s detection Fig. 4: Qualitative comparison of object detections. (a) Detec- tions from the baseline YOLOv8n model. (b) Detections from the final optimized model. outputs to confirm that the quantitative metrics correspond to reliable real-world performance. A side-by-side comparison of detections on a sample image from the COCO validation set is shown in Figure 4. The optimized model successfully and highly precisely detects the key object (airplane), as shown in Fig. 4. Interest- ingly, both the optimized model (b) and the baseline (a) have the same confidence score of 0.88. Additionally, there is no difference between the two in the bounding box localization. This qualitative data validates that the pruned layers were in fact architecturally redundant for this inference task and shows that the surgical removal of the C2f bottleneck layers did not affect the model’s feature extraction capabilities. VII. CONCLUSION The proposed hybrid compression pipeline improved YOLOv8n inference speed on the Raspberry Pi 5 from 3.08 FPS to 4.13 FPS—a 34% gain—while maintaining accuracy with only a 0.3% drop (final mAP@.5–.95 of 0.354 on COCO). The tuning phase proved essential for recovering per- formance after pruning, showing that significant architectural reductions can be made without major accuracy loss. By combining post-training INT8 quantization with pro- grammatic surgical pruning, this work delivers a complete, hardware-oriented optimization framework that bridges the gap between theoretical compression research and real-world embedded deployment. Future extensions may incorporate knowledge distillation to further enhance accuracy. The pipeline is well-suited for embedded vision tasks in smart grid monitoring, substation surveillance, and autonomous inspec- tion systems. REFERENCES [1] J. Yin, P. Huang, D. Xiao, and B. Zhang, ”A lightweight rice pest detection algorithm using improved attention mechanism and YOLOv8,” Agriculture, vol. 14, no. 7, Art. no. 1052, 2024. [2] J. Wang et al., ”Improved lightweight YOLOv8 model for rice disease detection in multi-scale scenarios,”Agronomy, vol. 15, no. 2, Art. no. 445, 2025. [3] Y . Ding, C. Jiang, L. Song, F. Liu, and Y . Tao, ”RVDR-YOLOv8: A weed target detection model based on improved YOLOv8,”Electronics, vol. 13, no. 11, Art. no. 2182, 2024. [4] Y . Li and S. Wang, ”EGM-YOLOv8: A lightweight ship detection model with efficient feature fusion and attention mechanisms,”Journal of Marine Science and Engineering, vol. 13, no. 4, Art. no. 757, 2025. [5] S. Liu, F. Shao, W. Chu, J. Dai, and H. Zhang, ”An improved YOLOv8- based lightweight attention mechanism for cross-scale feature fusion,” Remote Sensing, vol. 17, no. 6, Art. no. 1044, 2025. [6] B. Khalili and A. W. Smyth, ”SOD-YOLOv8: Enhancing YOLOv8 for small object detection in traffic scenes,”arXiv preprint, arXiv:2408.04786, 2024. [7] J. P. C. Henriques et al., ”Development of a robotic cell using a YOLOv8 vision system embedded in a Raspberry Pi,” inProc. 2024 12th Int. Conf. Control, Mechatronics and Automation (ICCMA), 2024, pp. 171–176. [8] B. A. Kumar et al., ”Real-time welding defect detection with YOLOv8 on Raspberry Pi,” inProc. 2025 5th Int. Conf. Trends Material Science and Inventive Materials (ICTMIM), 2025, pp. 766–771. [9] S. U. Islam, G. Ferraioli, and V . Pascazio, ”Tomato leaf detection, segmentation, and extraction in real-time environment for accurate disease detection,”AgriEngineering, vol. 7, no. 4, Art. no. 120, 2025. [10] Z. Cai et al., ”YOLOv8n-FAWL: Object Detection for Autonomous Driving Using YOLOv8 Network on Edge Devices,” IEEE Access, vol. 12, pp. 158377–158390, 2024. [11] Y . Li et al., ”PKD-YOLOv8: A Collaborative Pruning and Knowledge Distillation Framework for Lightweight Rapeseed Pest Detection,” Sen- sors, vol. 25, no. 16, p. 5004, 2025. [12] M. V . P. Kumar and R. Karthika, “Traffic Sign Detection
<<CHUNK>>
and Recog- nition with Deep CNN Using Raspberry Pi 4 in Real-time,” inProc. 2023 11th Region 10 Humanitarian Technology Conference (R10-HTC), IEEE, pp. 1–6, 2023. [13] Karthika, R., & Parameswaran, L. (2022). A novel convolutional neural network based architecture for object detection and recognition with an application to traffic sign recognition from road scenes. Pattern recognition and image analysis, 32(2), 351-362. [14] Ravindran, S., Srikanth, S., Raagul, T. S., Madhankumar, R. M., & Ganesan, M. (2025, March). Depth and Dimension Estimation Using Computer Vision. In 2025 International Conference on Wireless Com- munications Signal Processing and Networking (WiSPNET) (pp. 1-7). IEEE. [15] Behera, S., Bhardwaj, B., Rose, A., Hamdaan, M., & Ganesan, M. (2022). DriveSense: Adaptive System for Driving Behaviour Analysis and Ranking. In Machine Learning Techniques for Smart City Applica- tions: Trends and Solutions (pp. 45-58). Cham: Springer International Publishing.
<<CHUNK>>
